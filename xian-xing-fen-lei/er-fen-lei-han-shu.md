# 二分类函数

此函数对线性和非线性二分类都适用。

## 二分类函数

对率函数Logistic Function，即可以做为激活函数使用，又可以当作二分类函数使用。而在很多不太正规的文字材料中，把这两个概念混用了，比如下面这个说法：“我们在最后使用Sigmoid激活函数来做二分类”，这是不恰当的。在本书中，我们会根据不同的任务区分激活函数和分类函数这两个概念，在二分类任务中，叫做Logistic函数，而在作为激活函数时，叫做Sigmoid函数。

* 公式

$$Logistic(z) = \frac{1}{1 + e^{-z}} \rightarrow a$$

* 导数

$$Logistic'(z) = a(1 - a)$$

* 输入值域

$$(-\infty, \infty)$$

* 输出值域

$$(0,1)$$

* 函数图像（图6-2）

![&#x56FE;6-2 Logistic&#x51FD;&#x6570;&#x56FE;&#x50CF;](../.gitbook/assets/image%20%2886%29.png)

* 使用方式

此函数实际上是一个概率计算，它把$$(-\infty, \infty)$$之间的任何数字都压缩到$$(0,1)$$之间，返回一个概率值，这个概率值接近1时，认为是正例，否则认为是负例。

训练时，一个样本x在经过神经网络的最后一层的矩阵运算结果作为输入z，经过Logistic计算后，输出一个$$(0,1)$$之间的预测值。我们假设这个样本的标签值为0属于负类，如果其预测值越接近0，就越接近标签值，那么误差越小，反向传播的力度就越小。

推理时，我们预先设定一个阈值比如0.5，则当推理结果大于0.5时，认为是正类；小于0.5时认为是负类；等于0.5时，根据情况自己定义。阈值也不一定就是0.5，也可以是0.65等等，阈值越大，准确率越高，召回率越低；阈值越小则相反，准确度越低，召回率越高。

比如：

* input=2时，output=0.88，而0.88&gt;0.5，算作正例
* input=-1时，output=0.27，而0.27&lt;0.5，算作负例

## 正向传播

### 矩阵运算

$$ z=x \cdot w + b \tag{1} $$

### 分类计算

$$ a = Logistic(z)={1 \over 1 + e^{-z}} \tag{2} $$

### 损失函数计算

二分类交叉熵损失函数：

$$ loss(w,b) = -[y \ln a+(1-y) \ln(1-a)] \tag{3} $$

## 反向传播

### 求损失函数对a的偏导

$$ \frac{\partial loss}{\partial a}=-[{y \over a}+{-(1-y) \over 1-a}]=\frac{a-y}{a(1-a)} \tag{4} $$

### 求a对z的偏导

$$ \frac{\partial a}{\partial z}= a(1-a) \tag{5} $$

### 求损失函数loss对z的偏导

使用链式法则链接公式4和公式5：

$$
\frac{\partial loss}{\partial z}=\frac{\partial loss}{\partial a}\frac{\partial a}{\partial z}
$$

$$ =\frac{a-y}{a(1-a)} \cdot a(1-a)=a-y \tag{6} $$

我们惊奇地发现，使用交叉熵函数求导得到的分母，与Logistic分类函数求导后的结果，正好可以抵消，最后只剩下了$$a-y$$这一项。真的有这么巧合的事吗？实际上这是依靠科学家们的聪明才智寻找出了这种匹配关系，以满足以下条件：

1. 损失函数满足二分类的要求，无论是正例还是反例，都是单调的；
2. 损失函数可导，以便于使用反向传播算法；
3. 让计算过程非常简单，一个减法就可以搞定。

### 多样本情况

我们用三个样本做实例化推导：

$$
Z= \begin{pmatrix} z_1 \ z_2 \ z_3 \end{pmatrix} \\A=Logistic\begin{pmatrix} z_1 \ z_2 \ z_3 \end{pmatrix}= \begin{pmatrix} a_1 \ a_2 \ a_3 \end{pmatrix}
$$

$$
J(w, b)=-\left[y_{1} \ln a_{1}+\left(1-y_{1}\right) \ln \left(1-a_{1}\right)\right] \\ -\left[y_{2} \ln a_{2}+\left(1-y_{2}\right) \ln \left(1-a_{2}\right)\right]  \\ -\left[y_{3} \ln a_{3}+\left(1-y_{3}\right) \ln \left(1-a_{3}\right)\right]
$$

代入公式6结果： 

$$
\begin{aligned}
\frac{\partial J(w, b)}{\partial z} &=\left(\frac{\partial J(w, b)}{\partial z_{1}} \frac{\partial J(w, b)}{\partial z_{2}} \frac{\partial J(w, b)}{\partial z_{3}}\right) \\
&=\left(a_{1}-y_{1} \quad a_{2}-y_{2} \quad a_{3}-y_{3}\right) \\
&=A-Y
\end{aligned}
$$

所以，用矩阵运算时可以简化为矩阵相减的形式：$$A-Y$$。

## 对数几率的来历

经过数学推导后可以知道，神经网络实际也是在做这样一件事：经过调整w和b的值，把所有正例的样本都归纳到大于0.5的范围内，所有负例都小于0.5。但是如果只说大于或者小于，无法做准确的量化计算，所以用一个对率函数来模拟。

说到对率函数，还有一个问题，它为什么叫做“对数几率”函数呢？

我们举例说明：假设有一个硬币，抛出落地后，得到正面的概率是0.5，得到反面的概率是0.5，这两个概率叫做probability。如果用正面的概率除以反面的概率，0.5/0.5=1，这个数值叫做odds，即几率。

泛化一下，如果正面的概率是a，则反面的概率就是1-a，则几率等于：

$$odds = \frac{a}{1-a} \tag{9}$$

上式中，如果a是把样本x的预测为正例的可能性，那么1-a就是其负例的可能性，a/\(1-a\)就是正负例的比值，称为几率\(odds\)，反映了x作为正例的相对可能性，而对几率取对数就叫做对数几率\(log odds, logit\)。

假设概率如表6-3。

表6-3 概率到对数几率的对照表

| 概率a | 0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 反概率 \(1-a\) | 1 | 0.9 | 0.8 | 0.7 | 0.6 | 0.5 | 0.4 | 0.3 | 0.2 | 0.1 | 0 |
| 几率 odds | 0 | 0.11 | 0.25 | 0.43 | 0.67 | 1 | 1.5 | 2.33 | 4 | 9 | $$\infty$$ |
| 对数几率 ln\(odds\) | N/A | -2.19 | -1.38 | -0.84 | -0.4 | 0 | 0.4 | 0.84 | 1.38 | 2.19 | N/A |

可以看到几率的值不是线性的，不利于分析问题，所以在表中第4行对几率取对数，可以得到一组成线性关系的值，并可以用直线方程$$xw+b$$来表示，即：

$$ \ln(odds) = \ln \frac{a}{1-a}= xw + b \tag{10} $$

对公式10两边取自然指数：

$$ \frac{a}{1-a}=e^{xw+b} \tag{11} $$

$$
a=\frac{1}{1+e^{-(xw+b)}}
$$

令$$z=xw+b$$：

$$ a=\frac{1}{1+e^{-z}} \tag{12} $$

公式12就是公式2！对数几率的函数形式可以认为是这样得到的。

以上推导过程，实际上就是用线性回归模型的预测结果来逼近样本分类的对数几率。这就是为什么它叫做逻辑回归\(logistic regression\)，但其实是分类学习的方法。这种方法的优点如下：

* 直接对分类可能性建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题；
* 不仅预测出类别，而是得到了近似的概率，这对许多需要利用概率辅助决策的任务很有用；
* 对率函数是任意阶可导的凸函数，有很好的数学性，许多数值优化算法都可以直接用于求取最优解。

